\section{Background and Related Work}\label{sec:back}
This section details the type of problem \AlgName{} is designed to address; provides information about two critical components that are used in its construction; and gives a brief summary of another method which is similar to \AlgName{} from the literature.

\subsection{Problem type}
The algorithm presented here is designed to address bound-constrained, single-objective, multi-fidelity optimisation problems with continuous variables. Here, the terms bound-constrained and continuous imply that a variable may have an upper- or lower-bound which it cannot exceed, but that it may take any value (precision issues notwithstanding) between these bounds in the decision space; and that there are no regions in the objective space which are forbidden. 

Let $P$ be a problem instance with $D$ decision variables and let $\V{x} \in \prod^D_{i=1}[l_i,u_i]$ be a \emph{solution} to $P$, represented by a real vector\footnote{Bold type is used here to indicate a vector (indexed by superscript) and regular type is used for elements (indexed by subscript). E.g., $\V{x}^i$ is the $i$th indexed vector, and $x_j^i$ is the $j$th element of the $i$th vector.} $\V{x}$ such that $l_i \le x_i \le u_i$ for all ${i \in [D]}$, where $\V{l},\V{u} \in \mathbb{R}^D$ are the lower- and upper-bounds, respectively\footnote{To conserve space, the following shorthand is used in this paper: ${[k] = \{1,2,\dots,k\}}$ and $[k^*] = \{0,1,\dots,k-1\}$.}. The \emph{objective value} of a solution to $P$ is given by the function
\begin{equation}
f:\displaystyle\prod^D_{i=1}[l_i,u_i]\to \mathbb{R},\ \V{x} \mapsto f(\V{x})\,, \label{eq:objective}
\end{equation}
which is typically smooth. When this objective value is minimized, the so-called \emph{optimal} solution to $P$ is $\V{x}^*$, such that $f(\V{x}^*) \leq f(\V{x})$ for all $\V{x} \in \prod^D_{i=1}[l_i,u_i]$, although it may be maximized without loss of generality.

Examples of these types of problems abound in the field of engineering design~\cite{deb2012optimization,forrester2008engineering} and often they require some form of numerical simulation to compute the objective value of their solutions. This numerical simulation can be very computationally expensive, therefore researchers often employ artificial test functions when developing algorithms. While not generally being an accurate model for the behaviour of real-world problems, test functions are very fast to evaluate and can be customized to gauge the performance of algorithms on a variety of fitness landscapes.

% One advantage of numerical simulation is that the fidelity of evaluated objective values can often be controlled, trading accuracy of the simulation for computational budget~\cite{branke2016efficient}. This effect is difficult to model using test functions as a low-fidelity function must still have some correlation to the true function in order to be useful, but also must contain enough noise to obfuscate its behaviour. The experiments detailed in Section~\ref{sec:exp} use datasets generated by two different methods from the literature.

The dataset in Lv et al.~\cite{lv2021multi} uses the customized addition, removal and modification of terms to produce a transformed function with a desired correlation to the original.
% For example, the pair of high- and low-fidelity functions
% \begin{align*}
% f_H(\V{x}) &= \displaystyle\sum^2_{i=1}\left(x_i^4 - 16x_i^2 + 5x_i\right)\,,\\ 
% f_L(\V{x}) &= 0.5\displaystyle\sum^2_{i=1}\left(x_i^4 - 16x_i^2\right)\,, 
% \end{align*}
% have a correlation coefficient of $0.79$ over $\V{x} \in [0,1]^2$. 
While this method allows control over the shape of the fitness landscape, it requires analysing each test function individually and only permits a single level of fidelity for each transformation.

The second strategy introduces external noise which models the errors that occur when simulation fidelity is decreased. Wang et al.~\cite{wang2017generic} analysed the behaviour of many numerical simulations under different fidelity conditions and 
% the following three types of errors:
% \begin{enumerate}
% \item \emph{Resolution errors:} The landscape of resolution errors is multimodal due to the inconsistency of global and local errors. It is deterministic but gets smoother when the fidelity level increases.
% \item \emph{Stochastic errors:} The errors are stochastic, i.e., the fitness value of the same solution varies in different simulation runs. The stochastic error decreases as the fidelity level increases.
% \item \emph{Instability errors:} Representing failed simulations. A failure happens at a certain probability, and the probability decreases as the fidelity level increases.
% \end{enumerate}
% From this, they 
formulated a generic transformation function to turn any test function $f$ of the form in Equation~\ref{eq:objective} into a low-fidelity version $\tilde f$:
\begin{equation}
\tilde f: \displaystyle\prod^D_{i=1}[l_i,u_i] \times [0,10000] \to \mathbb{R},\ (\V{x},\phi) \mapsto f(\V{x}) + e(\V{x},\phi)\,,\label{eq:error-fn}
\end{equation}
% \begin{equation*}
% \tilde f(x,\phi) = f(x) + e(x,\phi),
% \end{equation*}
where $\phi$ is the \emph{fidelity level}, with $\tilde f(\V{x},10000) = f(\V{x})$ and $\tilde f(\V{x},0)$ having the worst possible correlation to $f(\V{x})$. The \emph{error function}, $e(\V{x},\phi)$ --- which returns a single real value --- can be one of ten different functions, all of which are independent of $f(\V{x})$.

Because the error function is always independent of $f(\V{x})$, this method can be applied to any test function at all; and because $\phi$ is real-valued, many different fidelity levels can be modelled easily--- although some analysis must still be performed if specific correlation coefficients are required.

\subsection{Kriging and co-kriging}

Originally arising from geostatistical methods used for ore valuation in mining research --- but since being applied across a number of domains --- is the so-called \emph{kriging} method~\cite{forrester2008engineering}. Kriging is an interpolation technique that uses a limited set of sampled data to predict the objective value at a point that has not been sampled yet.

Let $P$ be a problem instance with $D$ decision variables and let $X = \{\V{x}^1,\V{x}^2,\dots,\V{x}^n\}$ be a set of $n$ sample points with observed objective values $\V{y} = \{y_1,y_2,\dots,y_n\}$, where $\V{x}^i \in \mathbb{R}^D$ for $i \in [n]$. A kriging model of these sample data is the Gaussian process 
\begin{equation}
Y(\V{x}) = f(\V{x}) + Z(\V{x}),\label{eq:krig}
\end{equation}
where, $f(\V{x})$ is a polynomial regression function based on the sample data $\V{y}$ and encapsulates the main variations in the data. The function $Z(\V{x})$ is a Gaussian process with mean $0$, which models the residual error. As the mean of $Z(\V{x})$ is 0, $f(\V{x})$ is the mean of $Y(\V{x})$. 

% The correlation matrix $\Psi$ of $Y(\V{x})$ is
% \begin{equation}
% \text{cor}[Y(\V{x}^i),Y(\V{x}^l)] = \exp\left(-\displaystyle\sum^D_{j=1}\theta_j\mid x_j^i-x_j^l \mid ^{p_j}\right),
% \end{equation}
% where $p_j$ is a smoothness parameter (typically $p_j \in [1,2]$) and $\theta_j$ determines how much influence decision variable $j$ has for a given sample point. Both $\V{p}$ and $\V{\theta}$ have size $D$ and are usually determined using maximum likelihood estimation (MLE). Given the set of sample points $X$ and this correlation matrix $\Psi$, the predicted value $Y(\V{x}')$ can be computed using Equation~\ref{eq:krig}, employing regression to find $f(\V{x}')$ and by interpolation on $\Psi$ to find $Z(\V{x}')$.

% \medskip

Low-fidelity data is typically much cheaper to produce than high-fidelity data, a fact which the \emph{co-kriging} technique exploits to great effect. Adapted from Kennedy and O'Hagan's autoregressive model~\cite{kennedy2000predicting}, co-kriging correlates multiple sets of data with different fidelities to produce a single model that approximates the high-fidelity data. The residual error for a sample point evaluated at a given fidelity is recursively modelled as a function of the error of the same point evaluated at the fidelity below it --- until some foundational model with lowest fidelity is reached. Because of this, the Markov property must be assumed, that given a sample point evaluated at some fidelity, no more information about that point can be gained by evaluating it at a lower fidelity.

Let $X_H = \{\V{x}_H^1,\V{x}_H^2,\dots,\V{x}_H^n\}$ be the set of $n$ high-fidelity sample points with observed objective values $\V{y}_H = \{y_{H(1)},y_{H(2)},\dots,y_{H(n)}\}$ and $X_L = \{\V{x}_L^1,\V{x}_L^2,\dots,\V{x}_L^m\}$ be the set of $m$ low-fidelity\footnote{Although only two fidelity levels are employed here, without loss of generality, the method can be extended to an arbitrary number of fidelities.} sample points with observed objective values $\V{y}_L = \{y_{L(1)},y_{L(2)},\dots,y_{L(m)}\}$. A kriging model $Y_L(\V{x})$ of the low-fidelity data is constructed according to Equation~\ref{eq:krig}. Using this, the high-fidelity model is
\begin{equation}
Y_H(\V{x}) = \rho(\V{x})\mu_L(\V{x}) + Z_d(\V{x}),
\end{equation} 
where $\rho(\V{x})$ is a scaling factor, determined as part of the MLE of the second model; $\mu_L(\V{x})$ is the mean of $Y_L(\V{x})$; and $Z_d(\V{x})$ is a Gaussian process which models the difference between $Y_H(\V{x})$ and $\rho(\V{x})\mu_L(\V{x})$.

An in-depth mathematical treatment of kriging, co-kriging and how to use these models to make predictions can be found in~\cite{forrester2008engineering,forrester2007multi,kennedy2001bayesian,kennedy2000predicting}.

\subsection{Optimal computing budget allocation}

Stochastic simulation is a noisy process, requiring multiple simulation replications in order to accurately approximate the true fitness value of a given design. 
% This estimation becomes more accurate with an increased volume of replications; however, performing more replications consumes finite computing resources. Therefore, in the interest of efficiency, it is best to concentrate efforts on simulating promising designs.

Assuming a normal distribution, the mean fitness value for a design, $\mu$, is unknown, but it can be estimated by its sample mean $\hat{\mu}$. 
Given a set of $k$ candidate designs and a finite computing budget $T$, the optimal computing budget allocation (OCBA)~\cite{chen2011stochastic} method aims to find an allocation such that ${N_1 + N_2 + \dots + N_k = T}$, where $N_i$ is the total replications allocated to design $i$, in order to select the best design, $b \in [k]$, such that $\hat{\mu}_b < \hat{\mu}_i$ for all $i \in [k]$.

The probability that design $b$ actually \emph{is} the the best design is called the probability of correct selection (PCS).
% \begin{equation}
% PCS = P\{\tilde\mu_b < \tilde\mu_i,i \neq b\},
% \end{equation}
% where $\tilde\mu_i \sim \mathcal{N}\left(\hat{\mu}_i,\frac{\sigma_i^2}{N_i}\right)$ is the posterior distribution of the unknown mean of design $i$, $\mu_i$.
The PCS can be estimated using Monte Carlo simulation, but this is time-consuming; therefore, the following problem is formulated in~\cite{chen2011stochastic} to compute the approximate probability of correct selection (APCS):
\begin{align}
\underset{N_1,\dots,N_k}{\text{maximize}}&\quad 1 - \displaystyle\sum^k_{i=1,i\neq b}P\{\tilde\mu_b < \tilde\mu_i\}\,,\nonumber\\
\text{such that}& \quad \displaystyle\sum^k_{i=1}N_i = T\,, \quad N_i \geq 0,
\end{align}

Based on the work in~\cite{chen2011stochastic}, the APCS is asymptotically maximized (as $T \to \infty$) when
\begin{align}
\dfrac{N_i}{N_j} &= \left(\dfrac{\sigma_i/\delta_{b,i}}{\sigma_j/\delta_{b,j}}\right)^2\,,\label{eq:apcs1}\\[0.2cm]
N_b &= \sigma_b \sqrt{\displaystyle\sum^k_{i=1,i\neq b}\dfrac{N_i^2}{\sigma_i^2}}\,,\label{eq:apcs2}
\end{align}
where $N_i$ is the total replications allocated to design $i$ and $\delta_{b,i} = \hat{\mu}_b - \hat{\mu}_i$, for all $i,j\in \{1,2,\dots,k\}$ with $i \neq j \neq b$.

The implications of Equations~\ref{eq:apcs1} and~\ref{eq:apcs2} are such that additional computing resources are not only allocated to those designs with a small sample mean, but also to potentially promising designs that have a high variance. A high variance indicates uncertainty in the prediction, which increased volume of replications will help to address.

Using the above results, Algorithm~\ref{alg:ocba} details an iterative process to select a design, based on maximising APCS.

\begin{algorithm}[h!] 
\caption{$OCBA$ procedure}
\label{alg:ocba}
\algsetup{linenosize=\footnotesize}
{\footnotesize
\begin{algorithmic}[1]
\REQUIRE{$k$, number of designs; $T$, computing budget; $\Delta$, replications per update; $n_0$, initial replications.}
\ENSURE{$b$, index of best design.}
\STATE{$b \ot \emptyset$} \COMMENT{Initialize $b$}
\STATE{$N_i \ot n_0\,,\ \forall i \in [k]$} \COMMENT{Count initial replications}
\STATE{$\V{S}^i \ot Sim(n_0)\,,\ \forall i \in [k]$} \COMMENT{Perform initial replications}
\WHILE{$\displaystyle\sum_{i \in [k]} N_i < T$}\label{while-loop}
  \STATE{$\V{N}' \ot \V{N}$} \COMMENT{Store old allocations}
  \STATE{$\V{\hat{\mu}},\V{\hat{\sigma}} \ot Stats(\V{S}),\ \forall i \in [k]$} \COMMENT{Compute sample statistics}
  \STATE{$b \ot \text{argmin}_i\ \hat{\mu}_i,\ \forall i \in [k]$} \COMMENT{Update $b$}
  \STATE{$\V{N} \ot Allocate(\V{\hat{\mu}},\V{\hat{\sigma}},\V{N},\Delta)$} \COMMENT{Allocate $\Delta$ replications}
  \STATE{$S_i \ot Sim(N_i-N_i'),\ \forall i \in [k]$} \COMMENT{Perform additional simulations}
\ENDWHILE
\end{algorithmic}
}
\end{algorithm}

In this algorithm, $Sim(n)$ is a function that returns the output of running the stochastic simulation $n$ times; $Stats(\V{S})$ computes the sample statistics for a set of simulation outputs; and $Allocate(\V{\hat{\mu}},\V{\hat{\sigma}},\V{N},\Delta)$ is a function that allocates $\Delta$ replications among the designs, in accordance with Equations~\ref{eq:apcs1} and~\ref{eq:apcs2}.

\subsection{$MO^2TOS$}
Optimal computing budget allocation (OCBA) operates on the assumption that each stochastic simulation being performed will have the same input parameters, and does not consider the case where the fitness of a solution may be evaluated with different fidelities. The multi-fidelity optimisation with ordinal transformation and optimal sampling ($MO^2TOS$)~\cite{xu2016mo2tos} framework is a two-stage algorithm that addresses this consideration, by combining ideas from OCBA with ordinal transformation~\cite{xu2014ordinal}.

Given a problem instance $P$ and a finite computing budget $T$, the $MO^2TOS$ algorithm takes advantage of the relatively low cost of low-fidelity evaluations to identify promising regions of the search space that can subsequently be exploited by iterative high-fidelity evaluations.

A large population of solutions are evaluated in low-fidelity and then ranked by their evaluated value. This ranked population is then partitioned into equal-sized groups. The reason for ranking the solutions before partitioning, is to increase the probability that solutions which share a group are proximate to each other in the transformed objective space, as opposed to the original design space. As a result, regardless of where solutions may be, it is more likely that solutions within a group will have a similar performance.

These partitioned groups are treated as de facto ``designs'' for the purposes of OCBA allocation; however, instead of multiple simulation replications being performed on the same design, the allocations will determine how many solutions are selected from a group for high-fidelity evaluation.

Initially, each group has $n_0$ solutions selected --- without replacement --- and evaluated in high-fidelity. These results are used to determine the sample statistics of the groups. Equations~\ref{eq:apcs1} and~\ref{eq:apcs2} are used to allocate a portion of the total budget, $\Delta$, to the set of groups. Solutions are selected from these groups in accordance with this allocation, and evaluated in high-fidelity. The sample statistics are computed once again and the process repeats, while the total budget consumed is less than $T$.

The value of $\Delta$ is typically quite small, as there may be significant bias between the low- and high-fidelity evaluated data. By keeping $\Delta$ small, it ensures that the sample statistics of the groups are updated more frequently, meaning that the allocations are based more on the statistics of each group, as opposed to their intial partitioning.

The $MO^2TOS$ framework has some limitations, due to the fact that it samples the low-fidelity data \emph{a priori} and the fact that is a two-step process that operates directly on the high- and low-fidelity functions, as opposed to an iterative one. By sampling the data once, without any prior information, the sampling must occur uniformly across the entire design space. This means that unnecessary computational budget is likely to be expended in areas which are not helpful to the search. Similarly, it is necessary to initially evaluate $n_0$ solutions with high-fidelity from each partitioned group, which can result in unnecessary expenditure of computational budget as well.