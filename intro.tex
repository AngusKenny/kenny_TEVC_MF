\section{Introduction}
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.

% \hemant{IEEE requires use of US versions of the spelling ('-ize').}
\IEEEPARstart{M}{any} real-world engineering design problems require estimation of responses that are intractable for exact or analytical methods. In such cases, two alternatives are commonly resorted to: numerical simulations and physical testing of a prototype~\cite{forrester2008engineering}. When used in-loop during design optimization using iterative methods such as evolutionary algorithms~(EAs), both of these methods tend to be prohibitively slow; as well as potentially cost and resource intensive~\cite{jin2009systems}. Such optimization problems where each design evaluation incurs significant cost in any form are referred to as computationally expensive optimization problems.

Simulation-based optimization~(SO) refers to the methods that deal with optimization problems involving  numerical simulations\footnote{For brevity, the discussion is restricted to simulation-based design, but the same principles can be applied to other expensive optimization such as those involving physical prototyping.}. Typically, the these methods make use of surrogate approximations, also referred to as metamodels, to reduce the computational expense~\cite{amaran2016simulation}. The basic principle is to use historical information from previously evaluated designs to build a surrogate model of the response landscape. The predicted values from these surrogate models can be then utilized to determine new candidate solutions that are likely to be competent when evaluated using the true~(time consuming) simulation. Through this informed pre-selection, the number of expensive evaluations can be significantly reduced during the optimization. 

% A the production and testing of a prototype model can be probitively expensive, in terms of both time and cost. For these types of problem instances, numerical simulations --- such as finite element analysis (FEA) and computational fluid dynammics (CFD) --- can be employed to predict the outcome of various design choices. Simulation optimisation (SO) is an important tool which allows large design spaces to be searched, the merits of various candidate solutions to be approximated and inferences to be made about how those candidates might be improved --- all relatively cheaply.

% The principle behind SO is to iteratively use historical information from previous numerical simulations to construct a surrogate model (such as a kriging model) of the random field representing the fitness of a solution, with respect to its input variables. This model is searched using some optimisation technique, to find a new candidate --- or set of candidates --- which the model predicts will produce a good outcome when simulated. The data from this new simulation is incorporated into the surrogate model and the process continues.

% Although conducting a numerical simulation is not usually as expensive as producing and testing a physical prototype, it does incur a significant cost; with some complex FEA or CFD simulations taking several days to complete. Often, the allowed computational budget is given as a fixed amount, therefore it is important to ensure that this budget is used as efficiently as possible. In order to address this concern, the field of multi-fidelity simulation optimisation (MFSO) was developed.

Some numerical simulation processes allow control over the resolution, or fidelity, of the samples they produce. For example, in finite element analysis~(FEA) or computational fluid dynamic~(CFD) simulations, the mesh size can be controlled to yield solutions with different fidelities~\cite{branke2016efficient,toal2015some}. A coarse mesh yields a low-fidelity~(LF) performance estimate that is relatively fast but less accurate, while a fine mesh yields a high-fidelity~(HF) estimate that is relatively more time consuming but more accurate. Multi-fidelity SO methods~(MFSO) are a special class of SO methods that attempt to efficiently combine the information from different fidelities with an eventual aim of searching for optimal HF designs. 

% 
% However, by combining information from both high- and low-fidelity simulations, MFSO methods can produce good quality approximations for a much lower computational cost.

% FEA simulations this can be controlled by the mesh density, and in CFD simulations it can be controlled by the number of iterations the simulation is run for. While these lower-fidelity simulations are cheaper to produce, there is a trade-off in the accuracy of their predictions.

% However, by combining information from both high- and low-fidelity simulations, MFSO methods can produce good quality approximations for a much lower computational cost.
% \hemant{Insert all the references.}
A number of different approaches to MFSO have been explored in the literature. Many of these approaches are based on the co-kriging technique described by Forrester et al.~\cite{forrester2007multi} which correlates the two sets of samples, low- and high-fidelity, to produce a single prediction model. This technique is an extension of the autoregressive model first introduced by Kennedy and O'Hagan~\cite{kennedy2000predicting}. The two key aspects in which the different methods in this category deviate from each other, is in how they collect these samples and also how they search the model for potential candidates. Laurenceau and Sagaut~\cite{laurenceau2008building} investigated a number of different sampling methods for use with kriging and co-kriging in an airfoil design problem; Huang et al.~\cite{huang2013research} combine a genetic algorithm with co-kriging to optimize wing-body drag reduction in aerodynamic design; Perdikaris et al.~\cite{perdikaris2015multi} incorporate elements of statistical learning into a co-kriging framework to cross-correlate ensembles of multi-fidelity surrogate models; Yang et al.~\cite{yang2019physics} take a physics-informed approach, constructing models based on sparsely observed domain knowledge, representing unknowns as random variables or fields which are regressed using elements of co-kriging; and Giraldo et al.~\cite{giraldo2020cokriging} provide an extension to co-kriging for use when the secondary variable is functional, based on the work of Goulard and Voltz~\cite{goulard1993geostatistical}.

Among the approaches that do not use co-kriging models, some of the prominent ones include the following: Lv et al.~\cite{lv2021multi} employ a canonical correlation analysis-based model, in which the least squares method is used to determine optimal parameters; Ariyarit and Kanazaki~\cite{ariyarit2017multi} use a hybrid method which employs a kriging model to estimate local deviations and a radial basis function to approximate the global model in airfoil design problems; Hebbal et al.~\cite{hebbal2021multi} and Cutajar et al.~\cite{cutajar2019deep} both use machine learning techniques that treat the layers of a deep Gaussian process as different fidelity levels to capture non-linear correlations between fidelities; Xu et al.~\cite{xu2016mo2tos} use a two-stage process which first uses ordinal transformation to transform the original multi-dimensional design space into a one-dimensional ordinal space and then samples from this ordinal space using a method based on the optimal computational budget allocation (OCBA) algorithm proposed by Chen and Lee~\cite{chen2011stochastic}; Branke et al.~\cite{branke2016efficient} and Lim et al.~\cite{lim2008evolutionary} both take evolutionary approaches to solving MFSO problems; Bryson and Rumpfkeil~\cite{bryson2018multifidelity} propose a quasi-Newton method framework which can be used with many different surrogate model techniques; and Ng and Willcox~\cite{ng2014multifidelity} propose a number of approaches for multi-fidelity optimization under uncertainty.

Many of these approaches sample low- and high-fidelity solutions \emph{a priori}, then build models based on these samples and use some global search method to optimize them. To ensure these constructed models are properly representative, it is important to maintain a diversity of samples across the entire the design space; however, sampling without any prior knowledge can result in many computational resources being expended in areas which do not contain any promising candidates. Of those which do sample iteratively, information is typically only shared in one direction between the two datasets. Low-fidelity solutions are sampled randomly, or using some indepenent process, and used to inform where the high-fidelity solutions should be sampled from; but no information is then shared in the reverse direction, to inform the low-fidelity sampling in the next iteration. Again, this can result in computational resources being consumed unnecessarily in regions of the search space which are unproductive, especially in high-dimensional problems. 

% \hemant{Elaborate why that may be problematic}. 

To overcome these limitations and improve the performance for MFSO methods, this paper proposes an iterative two-stage, bound-constrained, single-objective multi-fidelity optimisation problems, referred to here as \AlgName{}. It uses previously obtained information about promising areas of the search space to define a restricted neighbourhood using a guided differential evolution (DE)~\cite{storn1997differential} process on a kriging model of the low-fidelity samples. This neighbourhood is then sampled from and searched, using a method derived from OCBA, to determine a set of candidates to undergo low-fidelity simulation. The information from these simulations is used to update the low-fidelity model and also a co-kriging-based surrogate model of the high-fidelity samples, which is searched globally using DE to find a suitable candidate for high-fidelity simulation. Finally, these high-fidelity samples are used to update the surrogate model and also to help determine the restricted neighbourhood in the next iteration. By using the high-fidelity simulation information to inform and restrict the region of interest while searching the low-fidelity model, \AlgName{} allows two-way information sharing between the sets of samples. The performance of the \AlgName{} model is compared against a baseline co-kriging-based MFSO algorithm on two separate datasets. The first is a common set of multi-fidelity test-functions from the literature, and the second is a set of multi-fidelity test functions that are generated from standard test functions using the methods described in the paper by Wang et al. In addition to this, some important properties of \AlgName{} are also investigated.

The remainder of this paper is organized as follows. Section~\ref{sec:back} provides the fundamentals and background of the proposed model, along with a description of the type of problems tackled and related work. The \AlgName{} algorithm is detailed in Section~\ref{sec:method}, describing all of its constituent parts and detailing some similarities and differences with a related technique from the literature. Experimental design and datasets are discussed in Section~\ref{sec:exp}, with Section~\ref{sec:results} giving the results of these experiments and a discussion of their implications. Finally, Section~\ref{sec:conc} provides the conclusion and outlines any future directions of research.

% \hemant{I note that the discussion of existing works has been quite short. If the target is TEVC, a strategic decision is required at this point - whether we are going for a `Letter' (up to 9 pages) or a regular paper (up to 15 pages). If regular paper, then a more extensive  literature review section is required, discussing a much larger array of works and some trends in the approaches. If letter, then perhaps we can get away with short discussion as per current Section~I, but we should still provide a clear motivation by identifying the limitations of the previous approaches. For Section II, there needs to be a substantial reduction in the discussion of what's known to be standard. This includes, for example, the mathematical formulae with regards to Kriging/co-kriging formulation. The overall section in my opinion should not be more than 1 page (give or take)}. 